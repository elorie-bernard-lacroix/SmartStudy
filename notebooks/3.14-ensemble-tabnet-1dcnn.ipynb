{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938cf7ff",
   "metadata": {},
   "source": [
    "# Experimenting with Ensembles\n",
    "This notebook goes over the possible combinations of models that we could use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1af36cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import MeanSquaredError, MeanAbsoluteError, R2Score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, make_scorer\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from torch.utils.data import TensorDataset, DataLoader  # Added missing imports\n",
    "from pytorch_lightning.callbacks import EarlyStopping  # Added missing import\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor  # Import TabNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0171ec77",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d517e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dataset\n",
    "#data = pd.read_csv(\"/content/drive/MyDrive/ECE324_Project/Model/dataset.csv\") # change path for your env\n",
    "#data = pd.read_csv(\"SmartStudy\\\\notebooks\\\\database.csv\") # change path for your env\n",
    "\n",
    "from smartstudy.config import PROCESSED_DATA_DIR\n",
    "data = pd.read_csv(PROCESSED_DATA_DIR / \"processed_data.csv\") # change path for your env\n",
    "# data = pd.read_csv(\"dataset.csv\") # change path for your env\n",
    "data.head()\n",
    "\n",
    "# Data Splitting & Normalization\n",
    "scaler = StandardScaler()\n",
    "input = data.drop(columns=['GPA'], errors='ignore')\n",
    "input = scaler.fit_transform(input)\n",
    "labels = data['GPA']\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(input, labels, test_size=0.3, random_state=42)\n",
    "X_test, X_val, Y_test, Y_val = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f48297",
   "metadata": {},
   "source": [
    "## Soft Ordering 1DCNN + TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c57e4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name          | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0  | batch_norm1   | BatchNorm1d       | 22     | train\n",
      "1  | dropout1      | Dropout           | 0      | train\n",
      "2  | dense1        | Linear            | 6.1 K  | train\n",
      "3  | batch_norm_c1 | BatchNorm1d       | 32     | train\n",
      "4  | conv1         | Conv1d            | 161    | train\n",
      "5  | ave_po_c1     | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2 | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2    | Dropout           | 0      | train\n",
      "8  | conv2         | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3 | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3    | Dropout           | 0      | train\n",
      "11 | dense2        | Linear            | 513    | train\n",
      "12 | mse           | MeanSquaredError  | 0      | train\n",
      "13 | mae           | MeanAbsoluteError | 0      | train\n",
      "14 | r2            | R2Score           | 0      | train\n",
      "-------------------------------------------------------------\n",
      "10.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.1 K    Total params\n",
      "0.040     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 53/53 [00:01<00:00, 42.54it/s, v_num=9]\n",
      "Mean Squared Error: 0.11332881944070748\n",
      "Mean Absolute Error: 0.26926889027341006\n",
      "R2 Score: 0.8596705561383823\n",
      "Kendall Tau: 0.8152222965717931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.03159 | val_0_rmse: 1.27661 |  0:00:00s\n",
      "epoch 1  | loss: 1.43149 | val_0_rmse: 1.19889 |  0:00:00s\n",
      "epoch 2  | loss: 1.16549 | val_0_rmse: 1.12089 |  0:00:00s\n",
      "epoch 3  | loss: 0.98182 | val_0_rmse: 1.01765 |  0:00:00s\n",
      "epoch 4  | loss: 0.87742 | val_0_rmse: 0.93789 |  0:00:01s\n",
      "epoch 5  | loss: 0.743   | val_0_rmse: 0.90094 |  0:00:01s\n",
      "epoch 6  | loss: 0.62199 | val_0_rmse: 0.88696 |  0:00:01s\n",
      "epoch 7  | loss: 0.56044 | val_0_rmse: 0.86854 |  0:00:01s\n",
      "epoch 8  | loss: 0.53052 | val_0_rmse: 0.77366 |  0:00:01s\n",
      "epoch 9  | loss: 0.44133 | val_0_rmse: 0.69411 |  0:00:01s\n",
      "epoch 10 | loss: 0.40083 | val_0_rmse: 0.63817 |  0:00:02s\n",
      "epoch 11 | loss: 0.39459 | val_0_rmse: 0.61472 |  0:00:02s\n",
      "epoch 12 | loss: 0.38203 | val_0_rmse: 0.58867 |  0:00:02s\n",
      "epoch 13 | loss: 0.32186 | val_0_rmse: 0.57388 |  0:00:02s\n",
      "epoch 14 | loss: 0.32932 | val_0_rmse: 0.57867 |  0:00:02s\n",
      "epoch 15 | loss: 0.31144 | val_0_rmse: 0.55841 |  0:00:02s\n",
      "epoch 16 | loss: 0.28097 | val_0_rmse: 0.54684 |  0:00:03s\n",
      "epoch 17 | loss: 0.28999 | val_0_rmse: 0.55724 |  0:00:03s\n",
      "epoch 18 | loss: 0.25031 | val_0_rmse: 0.54625 |  0:00:03s\n",
      "epoch 19 | loss: 0.24042 | val_0_rmse: 0.49265 |  0:00:03s\n",
      "epoch 20 | loss: 0.22203 | val_0_rmse: 0.46401 |  0:00:03s\n",
      "epoch 21 | loss: 0.22167 | val_0_rmse: 0.44275 |  0:00:03s\n",
      "epoch 22 | loss: 0.22663 | val_0_rmse: 0.42658 |  0:00:04s\n",
      "epoch 23 | loss: 0.18367 | val_0_rmse: 0.42343 |  0:00:04s\n",
      "epoch 24 | loss: 0.1868  | val_0_rmse: 0.44098 |  0:00:04s\n",
      "epoch 25 | loss: 0.17914 | val_0_rmse: 0.45907 |  0:00:04s\n",
      "epoch 26 | loss: 0.16691 | val_0_rmse: 0.4691  |  0:00:04s\n",
      "epoch 27 | loss: 0.16899 | val_0_rmse: 0.44039 |  0:00:04s\n",
      "epoch 28 | loss: 0.16107 | val_0_rmse: 0.40465 |  0:00:05s\n",
      "epoch 29 | loss: 0.14314 | val_0_rmse: 0.38384 |  0:00:05s\n",
      "epoch 30 | loss: 0.13297 | val_0_rmse: 0.37709 |  0:00:05s\n",
      "epoch 31 | loss: 0.13741 | val_0_rmse: 0.37324 |  0:00:05s\n",
      "epoch 32 | loss: 0.1197  | val_0_rmse: 0.37203 |  0:00:05s\n",
      "epoch 33 | loss: 0.12428 | val_0_rmse: 0.37474 |  0:00:05s\n",
      "epoch 34 | loss: 0.11886 | val_0_rmse: 0.37452 |  0:00:06s\n",
      "epoch 35 | loss: 0.11694 | val_0_rmse: 0.35959 |  0:00:06s\n",
      "epoch 36 | loss: 0.12544 | val_0_rmse: 0.36038 |  0:00:06s\n",
      "epoch 37 | loss: 0.11958 | val_0_rmse: 0.35557 |  0:00:06s\n",
      "epoch 38 | loss: 0.11427 | val_0_rmse: 0.3413  |  0:00:06s\n",
      "epoch 39 | loss: 0.1227  | val_0_rmse: 0.33879 |  0:00:06s\n",
      "epoch 40 | loss: 0.10432 | val_0_rmse: 0.33528 |  0:00:07s\n",
      "epoch 41 | loss: 0.10716 | val_0_rmse: 0.33514 |  0:00:07s\n",
      "epoch 42 | loss: 0.10695 | val_0_rmse: 0.33166 |  0:00:07s\n",
      "epoch 43 | loss: 0.1006  | val_0_rmse: 0.32714 |  0:00:07s\n",
      "epoch 44 | loss: 0.10295 | val_0_rmse: 0.32329 |  0:00:07s\n",
      "epoch 45 | loss: 0.10457 | val_0_rmse: 0.32222 |  0:00:08s\n",
      "epoch 46 | loss: 0.1079  | val_0_rmse: 0.31721 |  0:00:08s\n",
      "epoch 47 | loss: 0.10231 | val_0_rmse: 0.3237  |  0:00:08s\n",
      "epoch 48 | loss: 0.09594 | val_0_rmse: 0.32777 |  0:00:08s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type              | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | cnn_model | SoftOrdering1DCNN | 10.1 K | train\n",
      "--------------------------------------------------------\n",
      "10.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.1 K    Total params\n",
      "0.040     Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49 | loss: 0.10554 | val_0_rmse: 0.32701 |  0:00:08s\n",
      "Stop training because you reached max_epochs = 50 with best_epoch = 46 and best_val_0_rmse = 0.31721\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 47.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 53/53 [00:01<00:00, 27.58it/s, v_num=10]\n",
      "Ensemble Mean Squared Error: 0.07082640160815223\n",
      "Ensemble Mean Absolute Error: 0.20876302934490445\n",
      "Ensemble R2 Score: 0.9122991874666833\n",
      "Ensemble Kendall Tau: 0.8397472806212165\n"
     ]
    }
   ],
   "source": [
    "class SoftOrdering1DCNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim=1, sign_size=32, cha_input=16, cha_hidden=32, \n",
    "                 K=2, dropout_input=0.2, dropout_hidden=0.2, dropout_output=0.2, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_size = sign_size * cha_input\n",
    "        sign_size1 = sign_size\n",
    "        sign_size2 = sign_size // 2\n",
    "        output_size = (sign_size2) * cha_hidden\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cha_input = cha_input\n",
    "        self.cha_hidden = cha_hidden\n",
    "        self.K = K\n",
    "        self.sign_size1 = sign_size1\n",
    "        self.sign_size2 = sign_size2\n",
    "        self.output_size = output_size\n",
    "        self.dropout_input = dropout_input\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_output = dropout_output\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(input_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_input)\n",
    "        dense1 = nn.Linear(input_dim, hidden_size, bias=False)\n",
    "        self.dense1 = nn.utils.weight_norm(dense1)\n",
    "\n",
    "        # 1st conv layer\n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(cha_input)\n",
    "        conv1 = nn.Conv1d(\n",
    "            cha_input, \n",
    "            cha_input * K, \n",
    "            kernel_size=5, \n",
    "            stride=1, \n",
    "            padding=2,  \n",
    "            groups=cha_input, \n",
    "            bias=False)\n",
    "        self.conv1 = nn.utils.weight_norm(conv1, dim=None)\n",
    "\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size=sign_size2)\n",
    "\n",
    "        # 2nd conv layer\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(cha_input * K)\n",
    "        self.dropout_c2 = nn.Dropout(dropout_hidden)\n",
    "        conv2 = nn.Conv1d(\n",
    "            cha_input * K, \n",
    "            cha_hidden, \n",
    "            kernel_size=3, \n",
    "            stride=1, \n",
    "            padding=1, \n",
    "            bias=False)\n",
    "        self.conv2 = nn.utils.weight_norm(conv2, dim=None)\n",
    "\n",
    "        # 3rd conv layer (Output layer)\n",
    "        self.batch_norm_c3 = nn.BatchNorm1d(cha_hidden)\n",
    "        self.dropout_c3 = nn.Dropout(dropout_output)\n",
    "        self.dense2 = nn.Linear(output_size, output_dim) \n",
    "        \n",
    "        self.mse = MeanSquaredError()\n",
    "        self.mae = MeanAbsoluteError()\n",
    "        self.r2 = R2Score()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[1] != self.dense1.in_features:\n",
    "            raise ValueError(f\"Input feature size mismatch. Expected {self.dense1.in_features}, got {x.shape[1]}.\")\n",
    "\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.dense1(x))\n",
    "        \n",
    "        x = x.reshape(x.shape[0], self.cha_input, self.sign_size1) \n",
    "        \n",
    "        x = self.batch_norm_c1(x)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        \n",
    "        x = self.ave_po_c1(x)\n",
    "        \n",
    "        x = self.batch_norm_c2(x)\n",
    "        x = self.dropout_c2(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        \n",
    "        x = self.batch_norm_c3(x)\n",
    "        x = self.dropout_c3(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_mse', self.mse(y_hat, y))\n",
    "        self.log('val_mae', self.mae(y_hat, y))\n",
    "        self.log('val_r2', self.r2(y_hat, y))\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.float32).reshape(-1, 1) \n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.float32).reshape(-1, 1) \n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) \n",
    "test_loader = DataLoader(test_dataset, batch_size=32) \n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]  \n",
    "model = SoftOrdering1DCNN(input_dim=input_dim)\n",
    "\n",
    "# Configure Trainer and callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)  \n",
    "trainer = pl.Trainer(max_epochs=50, callbacks=[early_stopping]) \n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader, test_loader)  # Use train and validation loaders\n",
    "\n",
    "# Make predictions and evaluate\n",
    "predictions = []\n",
    "model.eval()  \n",
    "with torch.no_grad():\n",
    "    for x, _ in test_loader:\n",
    "        predictions.append(model(x))\n",
    "predictions = torch.cat(predictions).detach().numpy()\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "mse = mean_squared_error(Y_test, predictions)\n",
    "mae = mean_absolute_error(Y_test, predictions)\n",
    "r2 = r2_score(Y_test, predictions)\n",
    "kendall_tau_corr, _ = kendalltau(Y_test, predictions)\n",
    "\n",
    "print('Mean Squared Error:', mse)\n",
    "print('Mean Absolute Error:', mae)\n",
    "print('R2 Score:', r2)\n",
    "print('Kendall Tau:', kendall_tau_corr)\n",
    "\n",
    "class EnsembleModel(pl.LightningModule):\n",
    "    def __init__(self, cnn_model, tabnet_model, cnn_weight=0.5, tabnet_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.tabnet_model = tabnet_model\n",
    "        self.cnn_weight = cnn_weight\n",
    "        self.tabnet_weight = tabnet_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        cnn_pred = self.cnn_model(x)\n",
    "        tabnet_pred = self.tabnet_model.predict(x.numpy())  # TabNet expects numpy input\n",
    "        tabnet_pred = torch.tensor(tabnet_pred, dtype=torch.float32).to(cnn_pred.device)\n",
    "        return self.cnn_weight * cnn_pred + self.tabnet_weight * tabnet_pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# Reshape labels for TabNet compatibility\n",
    "Y_train = Y_train.values.reshape(-1, 1)  # Reshape to 2D\n",
    "Y_val = Y_val.values.reshape(-1, 1)      # Reshape to 2D\n",
    "\n",
    "# Instantiate TabNet model\n",
    "tabnet_model = TabNetRegressor()\n",
    "tabnet_model.fit(\n",
    "    X_train, Y_train,  # Use reshaped Y_train\n",
    "    eval_set=[(X_val, Y_val)],  # Use reshaped Y_val\n",
    "    eval_metric=['rmse'],\n",
    "    patience=5,\n",
    "    max_epochs=50\n",
    ")\n",
    "\n",
    "# Instantiate CNN model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "cnn_model = SoftOrdering1DCNN(input_dim=input_dim)\n",
    "\n",
    "# Instantiate Ensemble model\n",
    "ensemble_model = EnsembleModel(cnn_model, tabnet_model)\n",
    "\n",
    "# Train Ensemble model\n",
    "trainer = pl.Trainer(max_epochs=50, callbacks=[early_stopping])\n",
    "trainer.fit(ensemble_model, train_loader, test_loader)\n",
    "\n",
    "# Evaluate Ensemble model\n",
    "ensemble_predictions = []\n",
    "ensemble_model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, _ in test_loader:\n",
    "        ensemble_predictions.append(ensemble_model(x))\n",
    "ensemble_predictions = torch.cat(ensemble_predictions).detach().numpy()\n",
    "\n",
    "# Calculate and print evaluation metrics for ensemble\n",
    "mse = mean_squared_error(Y_test, ensemble_predictions)\n",
    "mae = mean_absolute_error(Y_test, ensemble_predictions)\n",
    "r2 = r2_score(Y_test, ensemble_predictions)\n",
    "kendall_tau_corr, _ = kendalltau(Y_test, ensemble_predictions)\n",
    "\n",
    "print('Ensemble Mean Squared Error:', mse)\n",
    "print('Ensemble Mean Absolute Error:', mae)\n",
    "print('Ensemble R2 Score:', r2)\n",
    "print('Ensemble Kendall Tau:', kendall_tau_corr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartstudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
