{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network MLP\n",
    "\n",
    "This notebook will focus on the implementation of an MLP to predict student GPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   StudentID  Age  Gender  Ethnicity  ParentalEducation  StudyTimeWeekly  \\\n",
      "0       1001   17       1          0                  2        19.833723   \n",
      "1       1002   18       0          0                  1        15.408756   \n",
      "2       1003   15       0          2                  3         4.210570   \n",
      "3       1004   17       1          0                  3        10.028829   \n",
      "4       1005   17       1          0                  2         4.672495   \n",
      "\n",
      "   Absences  Tutoring  ParentalSupport  Extracurricular  Sports  Music  \\\n",
      "0         7         1                2                0       0      1   \n",
      "1         0         0                1                0       0      0   \n",
      "2        26         0                2                0       0      0   \n",
      "3        14         0                3                1       0      0   \n",
      "4        17         1                3                0       0      0   \n",
      "\n",
      "   Volunteering       GPA  GradeClass  \n",
      "0             0  2.929196         2.0  \n",
      "1             0  3.042915         1.0  \n",
      "2             0  0.112602         4.0  \n",
      "3             0  2.054218         3.0  \n",
      "4             0  1.288061         4.0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('database.csv')\n",
    "print(df.head())\n",
    "\n",
    "# Data Splitting & Normalization\n",
    "scaler = StandardScaler()\n",
    "input = df.drop(columns=['GPA'], errors='ignore')\n",
    "input = scaler.fit_transform(input)\n",
    "labels = df['GPA']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(input, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results:\n",
      "MSE: 0.07033421930513462\n",
      "MAE: 0.21373674022761374\n",
      "R2: 0.9152543745793078\n",
      "Mean Absolute Error 0.2165654299941976\n",
      "Mean Squared Error: 0.07156730987170791\n",
      "Root Mean Squared Error: 0.26752067185865824\n",
      "R2 Score: 0.915605473349355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "# Convert X_train and Y_train to NumPy arrays if they are not already\n",
    "X_np = X_train.to_numpy() if hasattr(X_train, \"to_numpy\") else X_train\n",
    "Y_np = Y_train.to_numpy() if hasattr(Y_train, \"to_numpy\") else Y_train\n",
    "\n",
    "# Define the MLP model\n",
    "mlp_model = MLPRegressor(\n",
    "    hidden_layer_sizes=(64, 32),  # Two hidden layers with 64 and 32 neurons\n",
    "    activation='relu',  # Rectified Linear Unit activation function\n",
    "    solver='adam',  # Adam optimizer\n",
    "    max_iter=50,  # Maximum number of iterations (epochs)\n",
    "    random_state=42,  # Random state for reproducibility\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring_metrics = {\n",
    "    'MSE': mean_squared_error,\n",
    "    'MAE': mean_absolute_error,\n",
    "    'R2': r2_score\n",
    "}\n",
    "\n",
    "all_scores = {metric: [] for metric in scoring_metrics}\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_np):\n",
    "    X_cv_train, X_cv_val = X_np[train_idx], X_np[val_idx]\n",
    "    Y_cv_train, Y_cv_val = Y_np[train_idx], Y_np[val_idx]\n",
    "\n",
    "    mlp_model.fit(X_cv_train, Y_cv_train)\n",
    "    y_cv_pred = mlp_model.predict(X_cv_val)\n",
    "\n",
    "    for metric_name, metric_func in scoring_metrics.items():\n",
    "        all_scores[metric_name].append(metric_func(Y_cv_val, y_cv_pred))\n",
    "\n",
    "mean_scores = {metric: np.mean(scores) for metric, scores in all_scores.items()}\n",
    "\n",
    "# Print cross-validation results\n",
    "print('Cross-Validation Results:')\n",
    "for metric, score in mean_scores.items():\n",
    "    print(f'{metric}: {score}')\n",
    "\n",
    "# Train the model on the full training set\n",
    "mlp_model.fit(X_np, Y_np)\n",
    "\n",
    "# Predict on the test set\n",
    "mlp_pred = mlp_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae_model4 = mean_absolute_error(Y_test, mlp_pred)\n",
    "mse_model4 = mean_squared_error(Y_test, mlp_pred)\n",
    "rmse_model4 = np.sqrt(mse_model4)\n",
    "r2_model4 = r2_score(Y_test, mlp_pred)\n",
    "\n",
    "print('Mean Absolute Error', mae_model4)\n",
    "print('Mean Squared Error:', mse_model4)\n",
    "print('Root Mean Squared Error:', rmse_model4)\n",
    "print('R2 Score:', r2_model4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Reshape labels for TabNet compatibility\u001b[39;00m\n\u001b[32m      4\u001b[39m Y_train_tabnet = Y_train.values.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Reshape to 2D\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m Y_val_tabnet = \u001b[43mY_val\u001b[49m.values.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)      \u001b[38;5;66;03m# Reshape to 2D\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Instantiate TabNet model\u001b[39;00m\n\u001b[32m      8\u001b[39m tabnet_model = TabNetRegressor()\n",
      "\u001b[31mNameError\u001b[39m: name 'Y_val' is not defined"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# Reshape labels for TabNet compatibility\n",
    "Y_train_tabnet = Y_train.values.reshape(-1, 1)  # Reshape to 2D\n",
    "Y_val_tabnet = Y_val.values.reshape(-1, 1)      # Reshape to 2D\n",
    "\n",
    "# Instantiate TabNet model\n",
    "tabnet_model = TabNetRegressor()\n",
    "tabnet_model.fit(\n",
    "    X_train, Y_train_tabnet,  # Use reshaped Y_train\n",
    "    eval_set=[(X_val, Y_val_tabnet)],  # Use reshaped Y_val\n",
    "    eval_metric=['mse'],\n",
    "    patience=5,\n",
    "    max_epochs=50\n",
    ")\n",
    "\n",
    "tabnet_pred = tabnet_model.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "mae_model5 = mean_absolute_error(Y_test, tabnet_pred)\n",
    "mse_model5 = mean_squared_error(Y_test, tabnet_pred)\n",
    "rmse_model5 = np.sqrt(mse_model5)\n",
    "r2_model5 = r2_score(Y_test, tabnet_pred)\n",
    "\n",
    "print('Mean Absolute Error', mae_model5)\n",
    "print('Mean Squared Error:', mse_model5)\n",
    "print('Root Mean Squared Error:', rmse_model5)\n",
    "print('R2 Score:', r2_model5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabPFN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.029019828192856097\n",
      "Mean Absolute Error: 0.12807124210155396\n"
     ]
    }
   ],
   "source": [
    "# TabPFN\n",
    "from tabpfn import TabPFNRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Train and predict TabPFN\n",
    "reg = TabPFNRegressor(random_state=42)\n",
    "reg.fit(X_train, Y_train)\n",
    "Y3_pred = reg.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "print('Mean Squared Error:', mean_squared_error(Y_test, Y3_pred))\n",
    "print('Mean Absolute Error:', mean_absolute_error(Y_test, Y3_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D CNN with Soft Ordering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name          | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0  | batch_norm1   | BatchNorm1d       | 28     | train\n",
      "1  | dropout1      | Dropout           | 0      | train\n",
      "2  | dense1        | Linear            | 7.7 K  | train\n",
      "3  | batch_norm_c1 | BatchNorm1d       | 32     | train\n",
      "4  | conv1         | Conv1d            | 161    | train\n",
      "5  | ave_po_c1     | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2 | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2    | Dropout           | 0      | train\n",
      "8  | conv2         | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3 | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3    | Dropout           | 0      | train\n",
      "11 | dense2        | Linear            | 513    | train\n",
      "12 | mse           | MeanSquaredError  | 0      | train\n",
      "13 | mae           | MeanAbsoluteError | 0      | train\n",
      "14 | r2            | R2Score           | 0      | train\n",
      "-------------------------------------------------------------\n",
      "11.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.6 K    Total params\n",
      "0.046     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 48/48 [00:01<00:00, 37.35it/s, v_num=16]\n",
      "Mean Absolute Error 0.2458341706356151\n",
      "Mean Squared Error: 0.10626587304118576\n",
      "Root Mean Squared Error: 0.3259844674845502\n",
      "R2 Score: 0.8714939146938181\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchmetrics import MeanSquaredError, MeanAbsoluteError, R2Score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Loading Dataset\n",
    "data = pd.read_csv(\"database.csv\") # change path for your env\n",
    "#data = pd.read_csv(\"SmartStudy\\\\notebooks\\\\database.csv\") # change path for your env\n",
    "data.head()\n",
    "\n",
    "# Data Splitting & Normalization\n",
    "scaler = StandardScaler()\n",
    "input = data.drop(columns=['GPA'], errors='ignore')\n",
    "input = scaler.fit_transform(input)\n",
    "labels = data['GPA']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(input, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "class SoftOrdering1DCNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim=1, sign_size=32, cha_input=16, cha_hidden=32,\n",
    "                 K=2, dropout_input=0.2, dropout_hidden=0.2, dropout_output=0.2, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_size = sign_size * cha_input\n",
    "        sign_size1 = sign_size\n",
    "        sign_size2 = sign_size // 2\n",
    "        output_size = (sign_size2) * cha_hidden\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cha_input = cha_input\n",
    "        self.cha_hidden = cha_hidden\n",
    "        self.K = K\n",
    "        self.sign_size1 = sign_size1\n",
    "        self.sign_size2 = sign_size2\n",
    "        self.output_size = output_size\n",
    "        self.dropout_input = dropout_input\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_output = dropout_output\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(input_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_input)\n",
    "        dense1 = nn.Linear(input_dim, hidden_size, bias=False)\n",
    "        self.dense1 = nn.utils.weight_norm(dense1)\n",
    "\n",
    "        # 1st conv layer\n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(cha_input)\n",
    "        conv1 = nn.Conv1d(\n",
    "            cha_input,\n",
    "            cha_input * K,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=2,\n",
    "            groups=cha_input,\n",
    "            bias=False)\n",
    "        self.conv1 = nn.utils.weight_norm(conv1, dim=None)\n",
    "\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size=sign_size2)\n",
    "\n",
    "        # 2nd conv layer\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(cha_input * K)\n",
    "        self.dropout_c2 = nn.Dropout(dropout_hidden)\n",
    "        conv2 = nn.Conv1d(\n",
    "            cha_input * K,\n",
    "            cha_hidden,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False)\n",
    "        self.conv2 = nn.utils.weight_norm(conv2, dim=None)\n",
    "\n",
    "        # 3rd conv layer (Output layer)\n",
    "        self.batch_norm_c3 = nn.BatchNorm1d(cha_hidden)\n",
    "        self.dropout_c3 = nn.Dropout(dropout_output)\n",
    "        self.dense2 = nn.Linear(output_size, output_dim)\n",
    "\n",
    "        self.mse = MeanSquaredError()\n",
    "        self.mae = MeanAbsoluteError()\n",
    "        self.r2 = R2Score()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[1] != self.dense1.in_features:\n",
    "            raise ValueError(f\"Input feature size mismatch. Expected {self.dense1.in_features}, got {x.shape[1]}.\")\n",
    "\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.dense1(x))\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_input, self.sign_size1)\n",
    "\n",
    "        x = self.batch_norm_c1(x)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "\n",
    "        x = self.ave_po_c1(x)\n",
    "\n",
    "        x = self.batch_norm_c2(x)\n",
    "        x = self.dropout_c2(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "\n",
    "        x = self.batch_norm_c3(x)\n",
    "        x = self.dropout_c3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_mse', self.mse(y_hat, y))\n",
    "        self.log('val_mae', self.mae(y_hat, y))\n",
    "        self.log('val_r2', self.r2(y_hat, y))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "Y_val_tensor = torch.tensor(Y_val.values, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "CNN_model = SoftOrdering1DCNN(input_dim=input_dim)\n",
    "\n",
    "# Configure Trainer and callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "trainer = pl.Trainer(max_epochs=50, callbacks=[early_stopping]) # callbacks=[early_stopping]\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(CNN_model, train_loader, val_loader)  # Use train and validation loaders\n",
    "\n",
    "# Make predictions and evaluate\n",
    "predictions = []\n",
    "CNN_model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for x, _ in test_loader:\n",
    "        predictions.append(CNN_model(x))\n",
    "predictions = torch.cat(predictions).detach().numpy()\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "mse_model7 = mean_squared_error(Y_test, predictions)\n",
    "mae_model7 = mean_absolute_error(Y_test, predictions)\n",
    "rmse_model7 = np.sqrt(mse_model7)\n",
    "r2_model7 = r2_score(Y_test, predictions)\n",
    "\n",
    "print('Mean Absolute Error', mae_model7)\n",
    "print('Mean Squared Error:', mse_model7)\n",
    "print('Root Mean Squared Error:', rmse_model7)\n",
    "print('R2 Score:', r2_model7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartstudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
