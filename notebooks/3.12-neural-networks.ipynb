{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network MLP\n",
    "\n",
    "This notebook will focus on the implementation of an MLP to predict student GPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   StudentID  Age  Gender  Ethnicity  ParentalEducation  StudyTimeWeekly  \\\n",
      "0       1001   17       1          0                  2        19.833723   \n",
      "1       1002   18       0          0                  1        15.408756   \n",
      "2       1003   15       0          2                  3         4.210570   \n",
      "3       1004   17       1          0                  3        10.028829   \n",
      "4       1005   17       1          0                  2         4.672495   \n",
      "\n",
      "   Absences  Tutoring  ParentalSupport  Extracurricular  Sports  Music  \\\n",
      "0         7         1                2                0       0      1   \n",
      "1         0         0                1                0       0      0   \n",
      "2        26         0                2                0       0      0   \n",
      "3        14         0                3                1       0      0   \n",
      "4        17         1                3                0       0      0   \n",
      "\n",
      "   Volunteering       GPA  GradeClass  \n",
      "0             0  2.929196         2.0  \n",
      "1             0  3.042915         1.0  \n",
      "2             0  0.112602         4.0  \n",
      "3             0  2.054218         3.0  \n",
      "4             0  1.288061         4.0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('database.csv')\n",
    "print(df.head())\n",
    "\n",
    "# Data Splitting & Normalization\n",
    "scaler = StandardScaler()\n",
    "input = df.drop(columns=['GPA'], errors='ignore')\n",
    "input = scaler.fit_transform(input)\n",
    "labels = df['GPA']\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(input, labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Cross-Validation Results: ------\n",
      "MSE: 0.07748722526142184\n",
      "MAE: 0.22371758854943308\n",
      "R2: 0.9063242582814638\n",
      "------ Test Set Evaluation: ------\n",
      "Mean Absolute Error 0.2012109005116432\n",
      "Mean Squared Error: 0.06217013441386982\n",
      "Root Mean Squared Error: 0.249339396032536\n",
      "R2 Score: 0.9289550777907785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Convert X_train and Y_train to NumPy arrays if they are not already\n",
    "X_np = X_train.to_numpy() if hasattr(X_train, \"to_numpy\") else X_train\n",
    "Y_np = Y_train.to_numpy() if hasattr(Y_train, \"to_numpy\") else Y_train\n",
    "\n",
    "# Define the MLP model\n",
    "mlp_model = MLPRegressor(\n",
    "    hidden_layer_sizes=(64, 32),  # Two hidden layers with 64 and 32 neurons\n",
    "    activation='relu',  # Rectified Linear Unit activation function\n",
    "    solver='adam',  # Adam optimizer\n",
    "    max_iter=50,  # Maximum number of iterations (epochs)\n",
    "    random_state=42,  # Random state for reproducibility\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring_metrics = {\n",
    "    'MSE': mean_squared_error,\n",
    "    'MAE': mean_absolute_error,\n",
    "    'R2': r2_score\n",
    "}\n",
    "\n",
    "all_scores = {metric: [] for metric in scoring_metrics}\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_np):\n",
    "    X_cv_train, X_cv_val = X_np[train_idx], X_np[val_idx]\n",
    "    Y_cv_train, Y_cv_val = Y_np[train_idx], Y_np[val_idx]\n",
    "\n",
    "    mlp_model.fit(X_cv_train, Y_cv_train)\n",
    "    y_cv_pred = mlp_model.predict(X_cv_val)\n",
    "\n",
    "    for metric_name, metric_func in scoring_metrics.items():\n",
    "        all_scores[metric_name].append(metric_func(Y_cv_val, y_cv_pred))\n",
    "\n",
    "mean_scores = {metric: np.mean(scores) for metric, scores in all_scores.items()}\n",
    "\n",
    "# Print cross-validation results\n",
    "print('------ Cross-Validation Results: ------')\n",
    "for metric, score in mean_scores.items():\n",
    "    print(f'{metric}: {score}')\n",
    "\n",
    "# Train the model on the full training set\n",
    "mlp_model.fit(X_np, Y_np)\n",
    "\n",
    "# Predict on the test set\n",
    "mlp_pred = mlp_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae_model4 = mean_absolute_error(Y_test, mlp_pred)\n",
    "mse_model4 = mean_squared_error(Y_test, mlp_pred)\n",
    "rmse_model4 = np.sqrt(mse_model4)\n",
    "r2_model4 = r2_score(Y_test, mlp_pred)\n",
    "\n",
    "print('------ Test Set Evaluation: ------')\n",
    "print('Mean Absolute Error', mae_model4)\n",
    "print('Mean Squared Error:', mse_model4)\n",
    "print('Root Mean Squared Error:', rmse_model4)\n",
    "print('R2 Score:', r2_model4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 9.17376 | val_0_mse: 9.54685 |  0:00:00s\n",
      "epoch 1  | loss: 7.26037 | val_0_mse: 6.72676 |  0:00:00s\n",
      "epoch 2  | loss: 5.36512 | val_0_mse: 5.18418 |  0:00:00s\n",
      "epoch 3  | loss: 4.36627 | val_0_mse: 4.20072 |  0:00:00s\n",
      "epoch 4  | loss: 3.67885 | val_0_mse: 3.44159 |  0:00:00s\n",
      "epoch 5  | loss: 3.14873 | val_0_mse: 2.84822 |  0:00:01s\n",
      "epoch 6  | loss: 2.63231 | val_0_mse: 2.38738 |  0:00:01s\n",
      "epoch 7  | loss: 2.22859 | val_0_mse: 1.99877 |  0:00:01s\n",
      "epoch 8  | loss: 1.91147 | val_0_mse: 1.64181 |  0:00:01s\n",
      "epoch 9  | loss: 1.67334 | val_0_mse: 1.40845 |  0:00:01s\n",
      "epoch 10 | loss: 1.40459 | val_0_mse: 1.17822 |  0:00:01s\n",
      "epoch 11 | loss: 1.08523 | val_0_mse: 1.0235  |  0:00:02s\n",
      "epoch 12 | loss: 0.9216  | val_0_mse: 0.9095  |  0:00:02s\n",
      "epoch 13 | loss: 0.81488 | val_0_mse: 0.82757 |  0:00:02s\n",
      "epoch 14 | loss: 0.68797 | val_0_mse: 0.7607  |  0:00:02s\n",
      "epoch 15 | loss: 0.6085  | val_0_mse: 0.67604 |  0:00:02s\n",
      "epoch 16 | loss: 0.54757 | val_0_mse: 0.5865  |  0:00:03s\n",
      "epoch 17 | loss: 0.49691 | val_0_mse: 0.50046 |  0:00:03s\n",
      "epoch 18 | loss: 0.4065  | val_0_mse: 0.45618 |  0:00:03s\n",
      "epoch 19 | loss: 0.41318 | val_0_mse: 0.41908 |  0:00:03s\n",
      "epoch 20 | loss: 0.36121 | val_0_mse: 0.41281 |  0:00:03s\n",
      "epoch 21 | loss: 0.33744 | val_0_mse: 0.39816 |  0:00:03s\n",
      "epoch 22 | loss: 0.33575 | val_0_mse: 0.39392 |  0:00:04s\n",
      "epoch 23 | loss: 0.28552 | val_0_mse: 0.40401 |  0:00:04s\n",
      "epoch 24 | loss: 0.29314 | val_0_mse: 0.38461 |  0:00:04s\n",
      "epoch 25 | loss: 0.29341 | val_0_mse: 0.38358 |  0:00:04s\n",
      "epoch 26 | loss: 0.25742 | val_0_mse: 0.34278 |  0:00:04s\n",
      "epoch 27 | loss: 0.21948 | val_0_mse: 0.29441 |  0:00:04s\n",
      "epoch 28 | loss: 0.22666 | val_0_mse: 0.27105 |  0:00:05s\n",
      "epoch 29 | loss: 0.19706 | val_0_mse: 0.25967 |  0:00:05s\n",
      "epoch 30 | loss: 0.21016 | val_0_mse: 0.24959 |  0:00:05s\n",
      "epoch 31 | loss: 0.17709 | val_0_mse: 0.23471 |  0:00:05s\n",
      "epoch 32 | loss: 0.18315 | val_0_mse: 0.22108 |  0:00:05s\n",
      "epoch 33 | loss: 0.18811 | val_0_mse: 0.2094  |  0:00:05s\n",
      "epoch 34 | loss: 0.1681  | val_0_mse: 0.19672 |  0:00:06s\n",
      "epoch 35 | loss: 0.16996 | val_0_mse: 0.17757 |  0:00:06s\n",
      "epoch 36 | loss: 0.16364 | val_0_mse: 0.17206 |  0:00:06s\n",
      "epoch 37 | loss: 0.15136 | val_0_mse: 0.16754 |  0:00:06s\n",
      "epoch 38 | loss: 0.15443 | val_0_mse: 0.15927 |  0:00:06s\n",
      "epoch 39 | loss: 0.14522 | val_0_mse: 0.16061 |  0:00:07s\n",
      "epoch 40 | loss: 0.13261 | val_0_mse: 0.16119 |  0:00:07s\n",
      "epoch 41 | loss: 0.144   | val_0_mse: 0.153   |  0:00:07s\n",
      "epoch 42 | loss: 0.12608 | val_0_mse: 0.14708 |  0:00:07s\n",
      "epoch 43 | loss: 0.13936 | val_0_mse: 0.14072 |  0:00:08s\n",
      "epoch 44 | loss: 0.12634 | val_0_mse: 0.1343  |  0:00:08s\n",
      "epoch 45 | loss: 0.1264  | val_0_mse: 0.1319  |  0:00:08s\n",
      "epoch 46 | loss: 0.1304  | val_0_mse: 0.12729 |  0:00:08s\n",
      "epoch 47 | loss: 0.11865 | val_0_mse: 0.12344 |  0:00:08s\n",
      "epoch 48 | loss: 0.11664 | val_0_mse: 0.12175 |  0:00:08s\n",
      "epoch 49 | loss: 0.11702 | val_0_mse: 0.11653 |  0:00:09s\n",
      "Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_mse = 0.11653\n",
      "Mean Absolute Error 0.26128777042719614\n",
      "Mean Squared Error: 0.11948873888035635\n",
      "Root Mean Squared Error: 0.34567143197024014\n",
      "R2 Score: 0.8634542415153755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# Reshape labels for TabNet compatibility\n",
    "Y_train_tabnet = Y_train.values.reshape(-1, 1)  # Reshape to 2D\n",
    "Y_val_tabnet = Y_val.values.reshape(-1, 1)      # Reshape to 2D\n",
    "\n",
    "# Instantiate TabNet model\n",
    "tabnet_model = TabNetRegressor()\n",
    "tabnet_model.fit(\n",
    "    X_train, Y_train_tabnet,  # Use reshaped Y_train\n",
    "    eval_set=[(X_val, Y_val_tabnet)],  # Use reshaped Y_val\n",
    "    eval_metric=['mse'],\n",
    "    patience=5,\n",
    "    max_epochs=50\n",
    ")\n",
    "\n",
    "tabnet_pred = tabnet_model.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "mae_model5 = mean_absolute_error(Y_test, tabnet_pred)\n",
    "mse_model5 = mean_squared_error(Y_test, tabnet_pred)\n",
    "rmse_model5 = np.sqrt(mse_model5)\n",
    "r2_model5 = r2_score(Y_test, tabnet_pred)\n",
    "\n",
    "print('Mean Absolute Error', mae_model5)\n",
    "print('Mean Squared Error:', mse_model5)\n",
    "print('Root Mean Squared Error:', rmse_model5)\n",
    "print('R2 Score:', r2_model5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabPFN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.029019828192856097\n",
      "Mean Absolute Error: 0.12807124210155396\n"
     ]
    }
   ],
   "source": [
    "# TabPFN\n",
    "from tabpfn import TabPFNRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Train and predict TabPFN\n",
    "reg = TabPFNRegressor(random_state=42)\n",
    "reg.fit(X_train, Y_train)\n",
    "Y3_pred = reg.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "print('Mean Squared Error:', mean_squared_error(Y_test, Y3_pred))\n",
    "print('Mean Absolute Error:', mean_absolute_error(Y_test, Y3_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D CNN with Soft Ordering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name          | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0  | batch_norm1   | BatchNorm1d       | 28     | train\n",
      "1  | dropout1      | Dropout           | 0      | train\n",
      "2  | dense1        | Linear            | 7.7 K  | train\n",
      "3  | batch_norm_c1 | BatchNorm1d       | 32     | train\n",
      "4  | conv1         | Conv1d            | 161    | train\n",
      "5  | ave_po_c1     | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2 | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2    | Dropout           | 0      | train\n",
      "8  | conv2         | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3 | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3    | Dropout           | 0      | train\n",
      "11 | dense2        | Linear            | 513    | train\n",
      "12 | mse           | MeanSquaredError  | 0      | train\n",
      "13 | mae           | MeanAbsoluteError | 0      | train\n",
      "14 | r2            | R2Score           | 0      | train\n",
      "-------------------------------------------------------------\n",
      "11.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.6 K    Total params\n",
      "0.046     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 53.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 48/48 [00:01<00:00, 41.14it/s, v_num=17]\n",
      "Mean Absolute Error 0.2927950592641271\n",
      "Mean Squared Error: 0.13432998601856344\n",
      "Root Mean Squared Error: 0.3665105537615028\n",
      "R2 Score: 0.8375563090156952\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchmetrics import MeanSquaredError, MeanAbsoluteError, R2Score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Loading Dataset\n",
    "data = pd.read_csv(\"database.csv\") # change path for your env\n",
    "#data = pd.read_csv(\"SmartStudy\\\\notebooks\\\\database.csv\") # change path for your env\n",
    "data.head()\n",
    "\n",
    "# Data Splitting & Normalization\n",
    "scaler = StandardScaler()\n",
    "input = data.drop(columns=['GPA'], errors='ignore')\n",
    "input = scaler.fit_transform(input)\n",
    "labels = data['GPA']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(input, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "class SoftOrdering1DCNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim=1, sign_size=32, cha_input=16, cha_hidden=32,\n",
    "                 K=2, dropout_input=0.2, dropout_hidden=0.2, dropout_output=0.2, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_size = sign_size * cha_input\n",
    "        sign_size1 = sign_size\n",
    "        sign_size2 = sign_size // 2\n",
    "        output_size = (sign_size2) * cha_hidden\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cha_input = cha_input\n",
    "        self.cha_hidden = cha_hidden\n",
    "        self.K = K\n",
    "        self.sign_size1 = sign_size1\n",
    "        self.sign_size2 = sign_size2\n",
    "        self.output_size = output_size\n",
    "        self.dropout_input = dropout_input\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_output = dropout_output\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(input_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_input)\n",
    "        dense1 = nn.Linear(input_dim, hidden_size, bias=False)\n",
    "        self.dense1 = nn.utils.weight_norm(dense1)\n",
    "\n",
    "        # 1st conv layer\n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(cha_input)\n",
    "        conv1 = nn.Conv1d(\n",
    "            cha_input,\n",
    "            cha_input * K,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=2,\n",
    "            groups=cha_input,\n",
    "            bias=False)\n",
    "        self.conv1 = nn.utils.weight_norm(conv1, dim=None)\n",
    "\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size=sign_size2)\n",
    "\n",
    "        # 2nd conv layer\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(cha_input * K)\n",
    "        self.dropout_c2 = nn.Dropout(dropout_hidden)\n",
    "        conv2 = nn.Conv1d(\n",
    "            cha_input * K,\n",
    "            cha_hidden,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False)\n",
    "        self.conv2 = nn.utils.weight_norm(conv2, dim=None)\n",
    "\n",
    "        # 3rd conv layer (Output layer)\n",
    "        self.batch_norm_c3 = nn.BatchNorm1d(cha_hidden)\n",
    "        self.dropout_c3 = nn.Dropout(dropout_output)\n",
    "        self.dense2 = nn.Linear(output_size, output_dim)\n",
    "\n",
    "        self.mse = MeanSquaredError()\n",
    "        self.mae = MeanAbsoluteError()\n",
    "        self.r2 = R2Score()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[1] != self.dense1.in_features:\n",
    "            raise ValueError(f\"Input feature size mismatch. Expected {self.dense1.in_features}, got {x.shape[1]}.\")\n",
    "\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.dense1(x))\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_input, self.sign_size1)\n",
    "\n",
    "        x = self.batch_norm_c1(x)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "\n",
    "        x = self.ave_po_c1(x)\n",
    "\n",
    "        x = self.batch_norm_c2(x)\n",
    "        x = self.dropout_c2(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "\n",
    "        x = self.batch_norm_c3(x)\n",
    "        x = self.dropout_c3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_mse', self.mse(y_hat, y))\n",
    "        self.log('val_mae', self.mae(y_hat, y))\n",
    "        self.log('val_r2', self.r2(y_hat, y))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "Y_val_tensor = torch.tensor(Y_val.values, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "CNN_model = SoftOrdering1DCNN(input_dim=input_dim)\n",
    "\n",
    "# Configure Trainer and callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "trainer = pl.Trainer(max_epochs=50, callbacks=[early_stopping]) # callbacks=[early_stopping]\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(CNN_model, train_loader, val_loader)  # Use train and validation loaders\n",
    "\n",
    "# Make predictions and evaluate\n",
    "predictions = []\n",
    "CNN_model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for x, _ in test_loader:\n",
    "        predictions.append(CNN_model(x))\n",
    "predictions = torch.cat(predictions).detach().numpy()\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "mse_model7 = mean_squared_error(Y_test, predictions)\n",
    "mae_model7 = mean_absolute_error(Y_test, predictions)\n",
    "rmse_model7 = np.sqrt(mse_model7)\n",
    "r2_model7 = r2_score(Y_test, predictions)\n",
    "\n",
    "print('Mean Absolute Error', mae_model7)\n",
    "print('Mean Squared Error:', mse_model7)\n",
    "print('Root Mean Squared Error:', rmse_model7)\n",
    "print('R2 Score:', r2_model7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartstudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
