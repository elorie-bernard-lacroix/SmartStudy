{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938cf7ff",
   "metadata": {},
   "source": [
    "# Experimenting with Ensembles\n",
    "This notebook goes over the possible combinations of models that we could use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1af36cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import MeanSquaredError, MeanAbsoluteError, R2Score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, make_scorer\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from torch.utils.data import TensorDataset, DataLoader  # Added missing imports\n",
    "from pytorch_lightning.callbacks import EarlyStopping  # Added missing import\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor  # Import TabNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0171ec77",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d517e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dataset\n",
    "#data = pd.read_csv(\"/content/drive/MyDrive/ECE324_Project/Model/dataset.csv\") # change path for your env\n",
    "#data = pd.read_csv(\"SmartStudy\\\\notebooks\\\\database.csv\") # change path for your env\n",
    "data = pd.read_csv(\"dataset.csv\") # change path for your env\n",
    "data.head()\n",
    "\n",
    "# Data Splitting & Normalization\n",
    "scaler = StandardScaler()\n",
    "input = data.drop(columns=['GPA'], errors='ignore')\n",
    "input = scaler.fit_transform(input)\n",
    "labels = data['GPA']\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(input, labels, test_size=0.3, random_state=42)\n",
    "X_test, X_val, Y_test, Y_val = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f48297",
   "metadata": {},
   "source": [
    "## Soft Ordering 1DCNN + TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c57e4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name          | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0  | batch_norm1   | BatchNorm1d       | 20     | train\n",
      "1  | dropout1      | Dropout           | 0      | train\n",
      "2  | dense1        | Linear            | 5.6 K  | train\n",
      "3  | batch_norm_c1 | BatchNorm1d       | 32     | train\n",
      "4  | conv1         | Conv1d            | 161    | train\n",
      "5  | ave_po_c1     | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2 | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2    | Dropout           | 0      | train\n",
      "8  | conv2         | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3 | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3    | Dropout           | 0      | train\n",
      "11 | dense2        | Linear            | 513    | train\n",
      "12 | mse           | MeanSquaredError  | 0      | train\n",
      "13 | mae           | MeanAbsoluteError | 0      | train\n",
      "14 | r2            | R2Score           | 0      | train\n",
      "-------------------------------------------------------------\n",
      "9.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.6 K     Total params\n",
      "0.038     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 53/53 [00:01<00:00, 26.83it/s, v_num=0]\n",
      "Mean Squared Error: 0.15685813911031657\n",
      "Mean Absolute Error: 0.3136181722649528\n",
      "R2 Score: 0.8057703633096137\n",
      "Kendall Tau: 0.782200712718445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.26988 | val_0_rmse: 1.52637 |  0:00:00s\n",
      "epoch 1  | loss: 1.43018 | val_0_rmse: 1.25739 |  0:00:00s\n",
      "epoch 2  | loss: 1.09108 | val_0_rmse: 1.22301 |  0:00:00s\n",
      "epoch 3  | loss: 0.84355 | val_0_rmse: 1.18238 |  0:00:01s\n",
      "epoch 4  | loss: 0.79073 | val_0_rmse: 1.0447  |  0:00:01s\n",
      "epoch 5  | loss: 0.6575  | val_0_rmse: 0.92731 |  0:00:01s\n",
      "epoch 6  | loss: 0.55936 | val_0_rmse: 0.85949 |  0:00:02s\n",
      "epoch 7  | loss: 0.46131 | val_0_rmse: 0.82749 |  0:00:02s\n",
      "epoch 8  | loss: 0.40028 | val_0_rmse: 0.80773 |  0:00:02s\n",
      "epoch 9  | loss: 0.37541 | val_0_rmse: 0.76381 |  0:00:03s\n",
      "epoch 10 | loss: 0.35498 | val_0_rmse: 0.72976 |  0:00:03s\n",
      "epoch 11 | loss: 0.29711 | val_0_rmse: 0.66916 |  0:00:03s\n",
      "epoch 12 | loss: 0.30476 | val_0_rmse: 0.62699 |  0:00:04s\n",
      "epoch 13 | loss: 0.2657  | val_0_rmse: 0.57247 |  0:00:04s\n",
      "epoch 14 | loss: 0.2405  | val_0_rmse: 0.55886 |  0:00:04s\n",
      "epoch 15 | loss: 0.24222 | val_0_rmse: 0.53386 |  0:00:05s\n",
      "epoch 16 | loss: 0.22363 | val_0_rmse: 0.52516 |  0:00:05s\n",
      "epoch 17 | loss: 0.21863 | val_0_rmse: 0.51728 |  0:00:05s\n",
      "epoch 18 | loss: 0.20117 | val_0_rmse: 0.48103 |  0:00:06s\n",
      "epoch 19 | loss: 0.1868  | val_0_rmse: 0.4495  |  0:00:06s\n",
      "epoch 20 | loss: 0.16898 | val_0_rmse: 0.45583 |  0:00:06s\n",
      "epoch 21 | loss: 0.18299 | val_0_rmse: 0.42232 |  0:00:07s\n",
      "epoch 22 | loss: 0.16697 | val_0_rmse: 0.40459 |  0:00:07s\n",
      "epoch 23 | loss: 0.15733 | val_0_rmse: 0.40908 |  0:00:07s\n",
      "epoch 24 | loss: 0.17563 | val_0_rmse: 0.4026  |  0:00:07s\n",
      "epoch 25 | loss: 0.15972 | val_0_rmse: 0.40143 |  0:00:07s\n",
      "epoch 26 | loss: 0.1472  | val_0_rmse: 0.40024 |  0:00:08s\n",
      "epoch 27 | loss: 0.13199 | val_0_rmse: 0.39544 |  0:00:08s\n",
      "epoch 28 | loss: 0.13941 | val_0_rmse: 0.37924 |  0:00:08s\n",
      "epoch 29 | loss: 0.13554 | val_0_rmse: 0.36684 |  0:00:09s\n",
      "epoch 30 | loss: 0.12102 | val_0_rmse: 0.36474 |  0:00:09s\n",
      "epoch 31 | loss: 0.14718 | val_0_rmse: 0.35572 |  0:00:09s\n",
      "epoch 32 | loss: 0.13457 | val_0_rmse: 0.34796 |  0:00:09s\n",
      "epoch 33 | loss: 0.13054 | val_0_rmse: 0.34356 |  0:00:09s\n",
      "epoch 34 | loss: 0.13058 | val_0_rmse: 0.34367 |  0:00:10s\n",
      "epoch 35 | loss: 0.12472 | val_0_rmse: 0.34115 |  0:00:10s\n",
      "epoch 36 | loss: 0.12014 | val_0_rmse: 0.34368 |  0:00:10s\n",
      "epoch 37 | loss: 0.12546 | val_0_rmse: 0.34148 |  0:00:10s\n",
      "epoch 38 | loss: 0.11538 | val_0_rmse: 0.33765 |  0:00:11s\n",
      "epoch 39 | loss: 0.11505 | val_0_rmse: 0.33476 |  0:00:11s\n",
      "epoch 40 | loss: 0.11982 | val_0_rmse: 0.33039 |  0:00:11s\n",
      "epoch 41 | loss: 0.11594 | val_0_rmse: 0.32679 |  0:00:12s\n",
      "epoch 42 | loss: 0.1133  | val_0_rmse: 0.32585 |  0:00:12s\n",
      "epoch 43 | loss: 0.11891 | val_0_rmse: 0.3265  |  0:00:12s\n",
      "epoch 44 | loss: 0.11042 | val_0_rmse: 0.32726 |  0:00:12s\n",
      "epoch 45 | loss: 0.10547 | val_0_rmse: 0.33164 |  0:00:13s\n",
      "epoch 46 | loss: 0.11961 | val_0_rmse: 0.33187 |  0:00:13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type              | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | cnn_model | SoftOrdering1DCNN | 9.6 K  | train\n",
      "--------------------------------------------------------\n",
      "9.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.6 K     Total params\n",
      "0.038     Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47 | loss: 0.10168 | val_0_rmse: 0.32924 |  0:00:13s\n",
      "\n",
      "Early stopping occurred at epoch 47 with best_epoch = 42 and best_val_0_rmse = 0.32585\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 53/53 [00:03<00:00, 15.61it/s, v_num=1]\n",
      "Ensemble Mean Squared Error: 0.09633288412530863\n",
      "Ensemble Mean Absolute Error: 0.24918601451456238\n",
      "Ensemble R2 Score: 0.8807157780200571\n",
      "Ensemble Kendall Tau: 0.8079083736636529\n"
     ]
    }
   ],
   "source": [
    "class SoftOrdering1DCNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim=1, sign_size=32, cha_input=16, cha_hidden=32, \n",
    "                 K=2, dropout_input=0.2, dropout_hidden=0.2, dropout_output=0.2, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_size = sign_size * cha_input\n",
    "        sign_size1 = sign_size\n",
    "        sign_size2 = sign_size // 2\n",
    "        output_size = (sign_size2) * cha_hidden  # Corrected output size calculation\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cha_input = cha_input\n",
    "        self.cha_hidden = cha_hidden\n",
    "        self.K = K\n",
    "        self.sign_size1 = sign_size1\n",
    "        self.sign_size2 = sign_size2\n",
    "        self.output_size = output_size\n",
    "        self.dropout_input = dropout_input\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_output = dropout_output\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(input_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_input)\n",
    "        dense1 = nn.Linear(input_dim, hidden_size, bias=False)\n",
    "        self.dense1 = nn.utils.weight_norm(dense1)\n",
    "\n",
    "        # 1st conv layer\n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(cha_input)\n",
    "        conv1 = nn.Conv1d(\n",
    "            cha_input, \n",
    "            cha_input * K, \n",
    "            kernel_size=5, \n",
    "            stride=1, \n",
    "            padding=2,  \n",
    "            groups=cha_input, \n",
    "            bias=False)\n",
    "        self.conv1 = nn.utils.weight_norm(conv1, dim=None)\n",
    "\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size=sign_size2)\n",
    "\n",
    "        # 2nd conv layer\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(cha_input * K)\n",
    "        self.dropout_c2 = nn.Dropout(dropout_hidden)\n",
    "        conv2 = nn.Conv1d(\n",
    "            cha_input * K, \n",
    "            cha_hidden, \n",
    "            kernel_size=3, \n",
    "            stride=1, \n",
    "            padding=1, \n",
    "            bias=False)\n",
    "        self.conv2 = nn.utils.weight_norm(conv2, dim=None)\n",
    "\n",
    "        # 3rd conv layer (Output layer)\n",
    "        self.batch_norm_c3 = nn.BatchNorm1d(cha_hidden)\n",
    "        self.dropout_c3 = nn.Dropout(dropout_output)\n",
    "        self.dense2 = nn.Linear(output_size, output_dim)  # Corrected dense2 input size\n",
    "        \n",
    "        self.mse = MeanSquaredError()\n",
    "        self.mae = MeanAbsoluteError()\n",
    "        self.r2 = R2Score()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[1] != self.dense1.in_features:\n",
    "            raise ValueError(f\"Input feature size mismatch. Expected {self.dense1.in_features}, got {x.shape[1]}.\")\n",
    "\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.dense1(x))\n",
    "        \n",
    "        x = x.reshape(x.shape[0], self.cha_input, self.sign_size1) \n",
    "        \n",
    "        x = self.batch_norm_c1(x)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        \n",
    "        x = self.ave_po_c1(x)\n",
    "        \n",
    "        x = self.batch_norm_c2(x)\n",
    "        x = self.dropout_c2(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        \n",
    "        x = self.batch_norm_c3(x)\n",
    "        x = self.dropout_c3(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)  # Using MSE loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_mse', self.mse(y_hat, y))\n",
    "        self.log('val_mae', self.mae(y_hat, y))\n",
    "        self.log('val_r2', self.r2(y_hat, y))\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "# Assuming X_train, Y_train, X_test, Y_test are your data\n",
    "\n",
    "# 1. Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.float32).reshape(-1, 1) \n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.float32).reshape(-1, 1) \n",
    "\n",
    "# 2. Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) \n",
    "test_loader = DataLoader(test_dataset, batch_size=32) \n",
    "\n",
    "# 3. Instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]  \n",
    "model = SoftOrdering1DCNN(input_dim=input_dim)\n",
    "\n",
    "# 4. Configure Trainer and callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)  \n",
    "trainer = pl.Trainer(max_epochs=50, callbacks=[early_stopping]) \n",
    "\n",
    "# 5. Train the model\n",
    "trainer.fit(model, train_loader, test_loader)  # Use train and validation loaders\n",
    "\n",
    "# 6. Make predictions and evaluate\n",
    "# Removed the redundant modification of `dense2` after training\n",
    "predictions = []\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for x, _ in test_loader:\n",
    "        predictions.append(model(x))\n",
    "predictions = torch.cat(predictions).detach().numpy()\n",
    "\n",
    "# 7. Calculate and print evaluation metrics\n",
    "mse = mean_squared_error(Y_test, predictions)\n",
    "mae = mean_absolute_error(Y_test, predictions)\n",
    "r2 = r2_score(Y_test, predictions)\n",
    "kendall_tau_corr, _ = kendalltau(Y_test, predictions)\n",
    "\n",
    "print('Mean Squared Error:', mse)\n",
    "print('Mean Absolute Error:', mae)\n",
    "print('R2 Score:', r2)\n",
    "print('Kendall Tau:', kendall_tau_corr)\n",
    "\n",
    "class EnsembleModel(pl.LightningModule):\n",
    "    def __init__(self, cnn_model, tabnet_model, cnn_weight=0.5, tabnet_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.tabnet_model = tabnet_model\n",
    "        self.cnn_weight = cnn_weight\n",
    "        self.tabnet_weight = tabnet_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        cnn_pred = self.cnn_model(x)\n",
    "        tabnet_pred = self.tabnet_model.predict(x.numpy())  # TabNet expects numpy input\n",
    "        tabnet_pred = torch.tensor(tabnet_pred, dtype=torch.float32).to(cnn_pred.device)\n",
    "        return self.cnn_weight * cnn_pred + self.tabnet_weight * tabnet_pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# Reshape labels for TabNet compatibility\n",
    "Y_train = Y_train.values.reshape(-1, 1)  # Reshape to 2D\n",
    "Y_val = Y_val.values.reshape(-1, 1)      # Reshape to 2D\n",
    "\n",
    "# Instantiate TabNet model\n",
    "tabnet_model = TabNetRegressor()\n",
    "tabnet_model.fit(\n",
    "    X_train, Y_train,  # Use reshaped Y_train\n",
    "    eval_set=[(X_val, Y_val)],  # Use reshaped Y_val\n",
    "    eval_metric=['rmse'],\n",
    "    patience=5,\n",
    "    max_epochs=50\n",
    ")\n",
    "\n",
    "# Instantiate CNN model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "cnn_model = SoftOrdering1DCNN(input_dim=input_dim)\n",
    "\n",
    "# Instantiate Ensemble model\n",
    "ensemble_model = EnsembleModel(cnn_model, tabnet_model)\n",
    "\n",
    "# Train Ensemble model\n",
    "trainer = pl.Trainer(max_epochs=50, callbacks=[early_stopping])\n",
    "trainer.fit(ensemble_model, train_loader, test_loader)\n",
    "\n",
    "# Evaluate Ensemble model\n",
    "ensemble_predictions = []\n",
    "ensemble_model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, _ in test_loader:\n",
    "        ensemble_predictions.append(ensemble_model(x))\n",
    "ensemble_predictions = torch.cat(ensemble_predictions).detach().numpy()\n",
    "\n",
    "# Calculate and print evaluation metrics for ensemble\n",
    "mse = mean_squared_error(Y_test, ensemble_predictions)\n",
    "mae = mean_absolute_error(Y_test, ensemble_predictions)\n",
    "r2 = r2_score(Y_test, ensemble_predictions)\n",
    "kendall_tau_corr, _ = kendalltau(Y_test, ensemble_predictions)\n",
    "\n",
    "print('Ensemble Mean Squared Error:', mse)\n",
    "print('Ensemble Mean Absolute Error:', mae)\n",
    "print('Ensemble R2 Score:', r2)\n",
    "print('Ensemble Kendall Tau:', kendall_tau_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1e37c",
   "metadata": {},
   "source": [
    "## XGBoost + TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9762a72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Running on CPU with more than 1000 samples is not allowed by default due to slow performance.\nTo override this behavior, set the environment variable TABPFN_ALLOW_CPU_LARGE_DATASET=1.\nAlternatively, consider using a GPU or the tabpfn-client API: https://github.com/PriorLabs/tabpfn-client",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m ensemble_model = XGBoostTabPFNEnsemble(xgb_model, tabpfn_model)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Evaluate the ensemble model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mensemble_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m ensemble_predictions = ensemble_model.predict(X_test)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Calculate and print evaluation metrics\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mXGBoostTabPFNEnsemble.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mself\u001b[39m.xgb_model.fit(X, y)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtabpfn_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwds):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\tabpfn\\regressor.py:459\u001b[39m, in \u001b[36mTabPFNRegressor.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    449\u001b[39m X, y, feature_names_in, n_features_in = validate_Xy_fit(\n\u001b[32m    450\u001b[39m     X,\n\u001b[32m    451\u001b[39m     y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    456\u001b[39m     ignore_pretraining_limits=\u001b[38;5;28mself\u001b[39m.ignore_pretraining_limits,\n\u001b[32m    457\u001b[39m )\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, np.ndarray)\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m \u001b[43mcheck_cpu_warning\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_names_in \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;28mself\u001b[39m.feature_names_in_ = feature_names_in\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\tabpfn\\base.py:267\u001b[39m, in \u001b[36mcheck_cpu_warning\u001b[39m\u001b[34m(device, X)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_samples > \u001b[32m1000\u001b[39m:\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_cpu_override:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    268\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mRunning on CPU with more than 1000 samples is not allowed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    269\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mby default due to slow performance.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    270\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTo override this behavior, set the environment variable \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    271\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTABPFN_ALLOW_CPU_LARGE_DATASET=1.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    272\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAlternatively, consider using a GPU or the tabpfn-client API: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    273\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/PriorLabs/tabpfn-client\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    274\u001b[39m         )\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m num_samples > \u001b[32m200\u001b[39m:\n\u001b[32m    276\u001b[39m     warnings.warn(\n\u001b[32m    277\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRunning on CPU with more than 200 samples may be slow.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mConsider using a GPU or the tabpfn-client API: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    279\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/PriorLabs/tabpfn-client\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    280\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    281\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Running on CPU with more than 1000 samples is not allowed by default due to slow performance.\nTo override this behavior, set the environment variable TABPFN_ALLOW_CPU_LARGE_DATASET=1.\nAlternatively, consider using a GPU or the tabpfn-client API: https://github.com/PriorLabs/tabpfn-client"
     ]
    }
   ],
   "source": [
    "from tabpfn import TabPFNRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class XGBoostTabPFNEnsemble(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, xgb_model, tabpfn_model, xgb_weight=0.5, tabpfn_weight=0.5):\n",
    "        self.xgb_model = xgb_model\n",
    "        self.tabpfn_model = tabpfn_model\n",
    "        self.xgb_weight = xgb_weight\n",
    "        self.tabpfn_weight = tabpfn_weight\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.xgb_model.fit(X, y)\n",
    "        self.tabpfn_model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        xgb_pred = self.xgb_model.predict(X)\n",
    "        tabpfn_pred = self.tabpfn_model.predict(X)\n",
    "        return self.xgb_weight * xgb_pred + self.tabpfn_weight * tabpfn_pred\n",
    "\n",
    "# Instantiate XGBoost model\n",
    "best_params = {\n",
    "    'gamma': 0.0563056841989118,\n",
    "    'learning_rate': 0.10822466143464428,\n",
    "    'max_depth': int(4.469228010863449),\n",
    "    'min_child_weight': 8.445729116830403,\n",
    "    'n_estimators': int(228.70928755928722)\n",
    "}\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror',\n",
    "                                    random_state=42,\n",
    "                                    **best_params)\n",
    "\n",
    "# Instantiate TabPFN model\n",
    "tabpfn_model = TabPFNRegressor(device='cpu')  # Use 'cuda' if GPU is available\n",
    "\n",
    "# Instantiate the ensemble model\n",
    "ensemble_model = XGBoostTabPFNEnsemble(xgb_model, tabpfn_model)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "ensemble_model.fit(X_train, Y_train)\n",
    "ensemble_predictions = ensemble_model.predict(X_test)\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "mse = mean_squared_error(Y_test, ensemble_predictions)\n",
    "mae = mean_absolute_error(Y_test, ensemble_predictions)\n",
    "r2 = r2_score(Y_test, ensemble_predictions)\n",
    "kendall_tau_corr, _ = kendalltau(Y_test, ensemble_predictions)\n",
    "\n",
    "print('Ensemble Mean Squared Error:', mse)\n",
    "print('Ensemble Mean Absolute Error:', mae)\n",
    "print('Ensemble R2 Score:', r2)\n",
    "print('Ensemble Kendall Tau:', kendall_tau_corr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartstudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
