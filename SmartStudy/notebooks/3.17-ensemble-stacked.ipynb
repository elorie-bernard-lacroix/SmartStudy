{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87564c6",
   "metadata": {},
   "source": [
    "# Experimenting with Ensembles\n",
    "This notebook goes over a stacked ensemble model. It uses MLP, TabNet, TabPFN, and 1D CNN to predict a first layer of predictions. Then, the XGBoost model learns the difference between these model predictions and the true values. To make a final decision based on all four model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1af36cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import MeanSquaredError, MeanAbsoluteError, R2Score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, make_scorer\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from torch.utils.data import TensorDataset, DataLoader  # Added missing imports\n",
    "from pytorch_lightning.callbacks import EarlyStopping  # Added missing import\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor  # Import TabNet\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0171ec77",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d517e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dataset\n",
    "#data = pd.read_csv(\"/content/drive/MyDrive/ECE324_Project/Model/dataset.csv\") # change path for your env\n",
    "#data = pd.read_csv(\"SmartStudy\\\\notebooks\\\\database.csv\") # change path for your env\n",
    "data = pd.read_csv(\"dataset.csv\") # change path for your env\n",
    "data.head()\n",
    "\n",
    "# Data Splitting & Normalization\n",
    "scaler = StandardScaler()\n",
    "input = data.drop(columns=['GPA'], errors='ignore')\n",
    "input = scaler.fit_transform(input)\n",
    "labels = data['GPA']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(input, labels, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3740090",
   "metadata": {},
   "source": [
    "# Defining Individual Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e4835",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40ca3da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 0.20897831247848983\n",
      "Mean Squared Error: 0.07210100939388063\n",
      "Root Mean Squared Error: 0.2685163112249992\n",
      "R2 Score: 0.9143727412407547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Convert X_train and Y_train to NumPy arrays if they are not already\n",
    "X_np = X_train.to_numpy() if hasattr(X_train, \"to_numpy\") else X_train\n",
    "Y_np = Y_train.to_numpy() if hasattr(Y_train, \"to_numpy\") else Y_train\n",
    "\n",
    "# Define the MLP model\n",
    "mlp_model = MLPRegressor(\n",
    "    hidden_layer_sizes=(64, 32),  # Two hidden layers with 64 and 32 neurons\n",
    "    activation='relu',  # Rectified Linear Unit activation function\n",
    "    solver='adam',  # Adam optimizer\n",
    "    max_iter=50,  # Maximum number of iterations (epochs)\n",
    "    random_state=42,  # Random state for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model on the full training set\n",
    "mlp_model.fit(X_np, Y_np)\n",
    "\n",
    "# Predict on the test set\n",
    "mlp_pred = mlp_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae_model4 = mean_absolute_error(Y_test, mlp_pred)\n",
    "mse_model4 = mean_squared_error(Y_test, mlp_pred)\n",
    "rmse_model4 = np.sqrt(mse_model4)\n",
    "r2_model4 = r2_score(Y_test, mlp_pred)\n",
    "\n",
    "print('Mean Absolute Error', mae_model4)\n",
    "print('Mean Squared Error:', mse_model4)\n",
    "print('Root Mean Squared Error:', rmse_model4)\n",
    "print('R2 Score:', r2_model4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec609ed",
   "metadata": {},
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3f28c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.46359 | val_0_mse: 2.38933 |  0:00:00s\n",
      "epoch 1  | loss: 1.38248 | val_0_mse: 1.3858  |  0:00:01s\n",
      "epoch 2  | loss: 0.95323 | val_0_mse: 1.08576 |  0:00:02s\n",
      "epoch 3  | loss: 0.75773 | val_0_mse: 0.92515 |  0:00:03s\n",
      "epoch 4  | loss: 0.67954 | val_0_mse: 0.76127 |  0:00:04s\n",
      "epoch 5  | loss: 0.55031 | val_0_mse: 0.67934 |  0:00:04s\n",
      "epoch 6  | loss: 0.43664 | val_0_mse: 0.56256 |  0:00:05s\n",
      "epoch 7  | loss: 0.41001 | val_0_mse: 0.47633 |  0:00:06s\n",
      "epoch 8  | loss: 0.3317  | val_0_mse: 0.42273 |  0:00:06s\n",
      "epoch 9  | loss: 0.314   | val_0_mse: 0.36743 |  0:00:07s\n",
      "epoch 10 | loss: 0.29514 | val_0_mse: 0.32514 |  0:00:08s\n",
      "epoch 11 | loss: 0.24167 | val_0_mse: 0.31802 |  0:00:08s\n",
      "epoch 12 | loss: 0.22289 | val_0_mse: 0.33679 |  0:00:09s\n",
      "epoch 13 | loss: 0.20762 | val_0_mse: 0.34564 |  0:00:10s\n",
      "epoch 14 | loss: 0.2164  | val_0_mse: 0.32489 |  0:00:10s\n",
      "epoch 15 | loss: 0.19082 | val_0_mse: 0.3014  |  0:00:11s\n",
      "epoch 16 | loss: 0.18869 | val_0_mse: 0.25728 |  0:00:11s\n",
      "epoch 17 | loss: 0.17407 | val_0_mse: 0.23629 |  0:00:12s\n",
      "epoch 18 | loss: 0.1715  | val_0_mse: 0.21692 |  0:00:12s\n",
      "epoch 19 | loss: 0.15158 | val_0_mse: 0.20019 |  0:00:13s\n",
      "epoch 20 | loss: 0.14865 | val_0_mse: 0.18971 |  0:00:13s\n",
      "epoch 21 | loss: 0.14093 | val_0_mse: 0.17626 |  0:00:14s\n",
      "epoch 22 | loss: 0.13874 | val_0_mse: 0.16476 |  0:00:15s\n",
      "epoch 23 | loss: 0.1325  | val_0_mse: 0.1542  |  0:00:15s\n",
      "epoch 24 | loss: 0.13026 | val_0_mse: 0.14455 |  0:00:16s\n",
      "epoch 25 | loss: 0.13236 | val_0_mse: 0.13967 |  0:00:16s\n",
      "epoch 26 | loss: 0.12254 | val_0_mse: 0.13637 |  0:00:17s\n",
      "epoch 27 | loss: 0.11765 | val_0_mse: 0.13122 |  0:00:18s\n",
      "epoch 28 | loss: 0.11795 | val_0_mse: 0.12265 |  0:00:19s\n",
      "epoch 29 | loss: 0.11612 | val_0_mse: 0.11566 |  0:00:19s\n",
      "epoch 30 | loss: 0.10989 | val_0_mse: 0.10988 |  0:00:20s\n",
      "epoch 31 | loss: 0.11618 | val_0_mse: 0.10916 |  0:00:21s\n",
      "epoch 32 | loss: 0.11074 | val_0_mse: 0.10983 |  0:00:21s\n",
      "epoch 33 | loss: 0.10721 | val_0_mse: 0.10775 |  0:00:22s\n",
      "epoch 34 | loss: 0.11599 | val_0_mse: 0.10485 |  0:00:23s\n",
      "epoch 35 | loss: 0.1003  | val_0_mse: 0.10584 |  0:00:24s\n",
      "epoch 36 | loss: 0.1142  | val_0_mse: 0.1024  |  0:00:24s\n",
      "epoch 37 | loss: 0.1037  | val_0_mse: 0.10225 |  0:00:25s\n",
      "epoch 38 | loss: 0.09644 | val_0_mse: 0.10235 |  0:00:26s\n",
      "epoch 39 | loss: 0.09587 | val_0_mse: 0.09708 |  0:00:27s\n",
      "epoch 40 | loss: 0.0964  | val_0_mse: 0.09756 |  0:00:28s\n",
      "epoch 41 | loss: 0.09867 | val_0_mse: 0.09613 |  0:00:28s\n",
      "epoch 42 | loss: 0.0982  | val_0_mse: 0.09335 |  0:00:29s\n",
      "epoch 43 | loss: 0.09855 | val_0_mse: 0.0937  |  0:00:29s\n",
      "epoch 44 | loss: 0.09139 | val_0_mse: 0.0903  |  0:00:30s\n",
      "epoch 45 | loss: 0.08972 | val_0_mse: 0.08539 |  0:00:30s\n",
      "epoch 46 | loss: 0.09022 | val_0_mse: 0.08615 |  0:00:31s\n",
      "epoch 47 | loss: 0.0911  | val_0_mse: 0.08671 |  0:00:31s\n",
      "epoch 48 | loss: 0.09395 | val_0_mse: 0.0871  |  0:00:32s\n",
      "epoch 49 | loss: 0.09231 | val_0_mse: 0.0886  |  0:00:32s\n",
      "Stop training because you reached max_epochs = 50 with best_epoch = 45 and best_val_0_mse = 0.08539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 0.24966621616014217\n",
      "Mean Squared Error: 0.10014072848289853\n",
      "Root Mean Squared Error: 0.31645019905650007\n",
      "R2 Score: 0.8810727319599463\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_xs, X_val, Y_train_xs, Y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reshape labels for TabNet compatibility\n",
    "Y_train_tabnet = Y_train_xs.values.reshape(-1, 1)  # Reshape to 2D\n",
    "Y_val_tabnet = Y_val.values.reshape(-1, 1)      # Reshape to 2D\n",
    "\n",
    "# Instantiate TabNet model\n",
    "tabnet_model = TabNetRegressor()\n",
    "tabnet_model.fit(\n",
    "    X_train_xs, Y_train_tabnet,  # Use reshaped Y_train\n",
    "    eval_set=[(X_val, Y_val_tabnet)],  # Use reshaped Y_val\n",
    "    eval_metric=['mse'],\n",
    "    patience=5,\n",
    "    max_epochs=50\n",
    ")\n",
    "\n",
    "tabnet_pred = tabnet_model.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "mae_model5 = mean_absolute_error(Y_test, tabnet_pred)\n",
    "mse_model5 = mean_squared_error(Y_test, tabnet_pred)\n",
    "rmse_model5 = np.sqrt(mse_model5)\n",
    "r2_model5 = r2_score(Y_test, tabnet_pred)\n",
    "\n",
    "print('Mean Absolute Error', mae_model5)\n",
    "print('Mean Squared Error:', mse_model5)\n",
    "print('Root Mean Squared Error:', rmse_model5)\n",
    "print('R2 Score:', r2_model5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962c406",
   "metadata": {},
   "source": [
    "## TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from tabpfn import TabPFNRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# train\n",
    "reg = TabPFNRegressor(random_state=42)\n",
    "reg.fit(X_train, Y_train)\n",
    "\n",
    "# predict\n",
    "tabpfn_pred = reg.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "mse_model6 = mean_squared_error(Y_test, tabpfn_pred)\n",
    "mae_model6 = mean_absolute_error(Y_test, tabpfn_pred)\n",
    "rmse_model6 = np.sqrt(mse_model6)\n",
    "r2_model6 = reg.score(X_test, Y_test)\n",
    "\n",
    "print('Mean Absolute Error', mae_model6)\n",
    "print('Mean Squared Error:', mse_model6)\n",
    "print('Root Mean Squared Error:', rmse_model6)\n",
    "print('R2 Score:', r2_model6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1219c",
   "metadata": {},
   "source": [
    "## 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d36d414d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name          | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0  | batch_norm1   | BatchNorm1d       | 20     | train\n",
      "1  | dropout1      | Dropout           | 0      | train\n",
      "2  | dense1        | Linear            | 5.6 K  | train\n",
      "3  | batch_norm_c1 | BatchNorm1d       | 32     | train\n",
      "4  | conv1         | Conv1d            | 161    | train\n",
      "5  | ave_po_c1     | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2 | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2    | Dropout           | 0      | train\n",
      "8  | conv2         | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3 | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3    | Dropout           | 0      | train\n",
      "11 | dense2        | Linear            | 513    | train\n",
      "12 | mse           | MeanSquaredError  | 0      | train\n",
      "13 | mae           | MeanAbsoluteError | 0      | train\n",
      "14 | r2            | R2Score           | 0      | train\n",
      "-------------------------------------------------------------\n",
      "9.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.6 K     Total params\n",
      "0.038     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 10.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (37) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 37/37 [00:03<00:00, 12.33it/s, v_num=5]\n",
      "Mean Squared Error: 0.1980232705322774\n",
      "Mean Absolute Error: 0.357263783229402\n",
      "R2 Score: 0.764827289260413\n"
     ]
    }
   ],
   "source": [
    "class SoftOrdering1DCNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim=1, sign_size=32, cha_input=16, cha_hidden=32, \n",
    "                 K=2, dropout_input=0.2, dropout_hidden=0.2, dropout_output=0.2, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_size = sign_size * cha_input\n",
    "        sign_size1 = sign_size\n",
    "        sign_size2 = sign_size // 2\n",
    "        output_size = (sign_size2) * cha_hidden\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cha_input = cha_input\n",
    "        self.cha_hidden = cha_hidden\n",
    "        self.K = K\n",
    "        self.sign_size1 = sign_size1\n",
    "        self.sign_size2 = sign_size2\n",
    "        self.output_size = output_size\n",
    "        self.dropout_input = dropout_input\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_output = dropout_output\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(input_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_input)\n",
    "        dense1 = nn.Linear(input_dim, hidden_size, bias=False)\n",
    "        self.dense1 = nn.utils.weight_norm(dense1)\n",
    "\n",
    "        # 1st conv layer\n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(cha_input)\n",
    "        conv1 = nn.Conv1d(\n",
    "            cha_input, \n",
    "            cha_input * K, \n",
    "            kernel_size=5, \n",
    "            stride=1, \n",
    "            padding=2,  \n",
    "            groups=cha_input, \n",
    "            bias=False)\n",
    "        self.conv1 = nn.utils.weight_norm(conv1, dim=None)\n",
    "\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size=sign_size2)\n",
    "\n",
    "        # 2nd conv layer\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(cha_input * K)\n",
    "        self.dropout_c2 = nn.Dropout(dropout_hidden)\n",
    "        conv2 = nn.Conv1d(\n",
    "            cha_input * K, \n",
    "            cha_hidden, \n",
    "            kernel_size=3, \n",
    "            stride=1, \n",
    "            padding=1, \n",
    "            bias=False)\n",
    "        self.conv2 = nn.utils.weight_norm(conv2, dim=None)\n",
    "\n",
    "        # 3rd conv layer (Output layer)\n",
    "        self.batch_norm_c3 = nn.BatchNorm1d(cha_hidden)\n",
    "        self.dropout_c3 = nn.Dropout(dropout_output)\n",
    "        self.dense2 = nn.Linear(output_size, output_dim) \n",
    "        \n",
    "        self.mse = MeanSquaredError()\n",
    "        self.mae = MeanAbsoluteError()\n",
    "        self.r2 = R2Score()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[1] != self.dense1.in_features:\n",
    "            raise ValueError(f\"Input feature size mismatch. Expected {self.dense1.in_features}, got {x.shape[1]}.\")\n",
    "\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.dense1(x))\n",
    "        \n",
    "        x = x.reshape(x.shape[0], self.cha_input, self.sign_size1) \n",
    "        \n",
    "        x = self.batch_norm_c1(x)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        \n",
    "        x = self.ave_po_c1(x)\n",
    "        \n",
    "        x = self.batch_norm_c2(x)\n",
    "        x = self.dropout_c2(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        \n",
    "        x = self.batch_norm_c3(x)\n",
    "        x = self.dropout_c3(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_mse', self.mse(y_hat, y))\n",
    "        self.log('val_mae', self.mae(y_hat, y))\n",
    "        self.log('val_r2', self.r2(y_hat, y))\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "Y_train_numpy = Y_train_xs.to_numpy().reshape(-1, 1)\n",
    "Y_val_numpy = Y_val.to_numpy().reshape(-1, 1)\n",
    "Y_test_numpy = Y_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_xs, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train_numpy, dtype=torch.float32).reshape(-1, 1) \n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "Y_val_tensor = torch.tensor(Y_val_numpy, dtype=torch.float32).reshape(-1, 1) \n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test_numpy, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) \n",
    "val_loader = DataLoader(val_dataset, batch_size=32) \n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]  \n",
    "cnn_model = SoftOrdering1DCNN(input_dim=input_dim)\n",
    "\n",
    "# Configure Trainer and callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)  \n",
    "trainer = pl.Trainer(max_epochs=50, callbacks=[early_stopping]) \n",
    "\n",
    "# Train the model\n",
    "trainer.fit(cnn_model, train_loader, val_loader)  # Use train and validation loaders\n",
    "\n",
    "# Make predictions and evaluate\n",
    "predictions = []\n",
    "cnn_model.eval()  \n",
    "with torch.no_grad():\n",
    "    for x, _ in test_loader:\n",
    "        predictions.append(cnn_model(x))\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "mse = mean_squared_error(Y_test, predictions)\n",
    "mae = mean_absolute_error(Y_test, predictions)\n",
    "r2 = r2_score(Y_test, predictions)\n",
    "kendall_tau_corr, _ = kendalltau(Y_test, predictions)\n",
    "\n",
    "print('Mean Squared Error:', mse)\n",
    "print('Mean Absolute Error:', mae)\n",
    "print('R2 Score:', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa1beb0",
   "metadata": {},
   "source": [
    "## XGBoost Ensemble Model\n",
    "This ensemble model combines predictions from TabNet, 1DCNN, MLP, and TabPFN using XGBoost to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0226d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2, 3, 4] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2, 3, 4] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2, 3, 4] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\eblac\\anaconda3\\envs\\smartstudy_env\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2, 3, 4] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Generate predictions from individual models\n",
    "tabnet_predictions = tabnet_model.predict(X_test)\n",
    "cnn_predictions = cnn_model(torch.tensor(X_test , dtype=torch.float32))\n",
    "mlp_predictions = mlp_model.predict(torch.tensor(X_test, dtype=torch.float32))\n",
    "tabpfn_predictions = reg.predict(torch.tensor(X_test, dtype=torch.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b85843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Ensemble Mean Squared Error: 7.596135259738206e-05\n",
      "XGBoost Ensemble Mean Absolute Error: 0.00600302039723602\n",
      "XGBoost Ensemble R2 Score: 0.9999059408329004\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Flatten predictions to 1D numpy arrays\n",
    "tabnet_predictions_shaped = tabnet_predictions.flatten()  # Already a numpy array\n",
    "cnn_predictions_shaped = cnn_predictions.flatten().detach().numpy()  # Detach and convert to numpy\n",
    "mlp_predictions_shaped = mlp_predictions.flatten()  # Detach and convert to numpy\n",
    "tabpfn_predictions_shaped = tabpfn_predictions.flatten() # Detach and convert to numpy\n",
    "\n",
    "# Combine predictions into a DataFrame\n",
    "ensemble_input = pd.DataFrame({\n",
    "    'TabNet': tabnet_predictions_shaped,\n",
    "    '1DCNN': cnn_predictions_shaped,\n",
    "    'MLP': mlp_predictions_shaped,\n",
    "    'TabPFN': tabpfn_predictions_shaped\n",
    "})\n",
    "\n",
    "# Train XGBoost on the combined predictions\n",
    "xgb_ensemble = XGBRegressor()\n",
    "xgb_ensemble.fit(ensemble_input, Y_test)\n",
    "\n",
    "# Make final predictions\n",
    "final_predictions = xgb_ensemble.predict(ensemble_input)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "mse = mean_squared_error(Y_test, final_predictions)\n",
    "mae = mean_absolute_error(Y_test, final_predictions)\n",
    "r2 = r2_score(Y_test, final_predictions)\n",
    "\n",
    "print('XGBoost Ensemble Mean Squared Error:', mse)\n",
    "print('XGBoost Ensemble Mean Absolute Error:', mae)\n",
    "print('XGBoost Ensemble R2 Score:', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713ca2bc",
   "metadata": {},
   "source": [
    "## Correlation Heatmap\n",
    "Visualizing the correlations between predictions from TabNet, 1DCNN, MLP, and TabPFN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b2bc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Calculate correlations\n",
    "correlation_matrix = ensemble_input.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap of Model Predictions')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartstudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
